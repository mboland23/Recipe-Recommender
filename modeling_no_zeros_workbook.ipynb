{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from surprise import Dataset, Reader, accuracy, NormalPredictor, KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline, SVD, BaselineOnly, SVDpp, NMF, SlopeOne, CoClustering\n",
    "from surprise.accuracy import rmse\n",
    "from surprise.prediction_algorithms import SVD, SVDpp, NMF, BaselineOnly, NormalPredictor\n",
    "from IPython.core.display import HTML\n",
    "from surprise.model_selection.split import train_test_split as surprise_train_test_split\n",
    "from surprise.model_selection import GridSearchCV, cross_validate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_ratings_df = pd.read_csv(\"./Data/user_reviews_no_zero.csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>recipe_id</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>56680</td>\n",
       "      <td>79222</td>\n",
       "      <td>2006-11-11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Oh, This was wonderful!  Had a soup and salad ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>183565</td>\n",
       "      <td>79222</td>\n",
       "      <td>2006-02-13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wow!  My family loves this recipe and it is a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>101823</td>\n",
       "      <td>79222</td>\n",
       "      <td>2006-03-21</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Excellent chowder.  This was the perfect warm-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>446143</td>\n",
       "      <td>79222</td>\n",
       "      <td>2008-02-01</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Oh, how wonderful!  I doubled the crab, and ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>226989</td>\n",
       "      <td>79222</td>\n",
       "      <td>2008-03-07</td>\n",
       "      <td>4.0</td>\n",
       "      <td>DH and I enjoyed this. However I used it only ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  recipe_id        date  rating  \\\n",
       "10    56680      79222  2006-11-11     5.0   \n",
       "11   183565      79222  2006-02-13     5.0   \n",
       "12   101823      79222  2006-03-21     5.0   \n",
       "13   446143      79222  2008-02-01     4.0   \n",
       "14   226989      79222  2008-03-07     4.0   \n",
       "\n",
       "                                               review  \n",
       "10  Oh, This was wonderful!  Had a soup and salad ...  \n",
       "11  Wow!  My family loves this recipe and it is a ...  \n",
       "12  Excellent chowder.  This was the perfect warm-...  \n",
       "13  Oh, how wonderful!  I doubled the crab, and ad...  \n",
       "14  DH and I enjoyed this. However I used it only ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 537267 entries, 10 to 598157\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    537267 non-null  int64  \n",
      " 1   recipe_id  537267 non-null  int64  \n",
      " 2   date       537267 non-null  object \n",
      " 3   rating     537267 non-null  float64\n",
      " 4   review     537267 non-null  object \n",
      "dtypes: float64(1), int64(2), object(2)\n",
      "memory usage: 24.6+ MB\n"
     ]
    }
   ],
   "source": [
    "user_ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 537267 entries, 10 to 598157\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    537267 non-null  object \n",
      " 1   recipe_id  537267 non-null  object \n",
      " 2   date       537267 non-null  object \n",
      " 3   rating     537267 non-null  float64\n",
      " 4   review     537267 non-null  object \n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 24.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#df[['one', 'two', 'three']] = df[['one', 'two', 'three']]. astype(str) #\n",
    "user_ratings_df[[\"user_id\", \"recipe_id\"]] = user_ratings_df[[\"user_id\", \"recipe_id\"]].astype(str)\n",
    "user_ratings_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40526"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up data frame for Surprise using user_id, recipe_id, and rating\n",
    "rating_surprise_df = user_ratings_df[[\"user_id\", \"recipe_id\", \"rating\"]]\n",
    "len(rating_surprise_df[\"recipe_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 537267 entries, 10 to 598157\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    537267 non-null  object \n",
      " 1   recipe_id  537267 non-null  object \n",
      " 2   rating     537267 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 16.4+ MB\n"
     ]
    }
   ],
   "source": [
    "rating_surprise_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17096"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rating_surprise_df[\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1,5))\n",
    "surprise_data = Dataset.load_from_df(rating_surprise_df, reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x7fc848ca91c0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surprise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up train_test_split \n",
    "trainset, testset = surprise_train_test_split(surprise_data, test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.7614\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the NormalPredictor() from Surprise\n",
    "baseline = NormalPredictor()\n",
    "\n",
    "#Fit on the trainset\n",
    "baseline.fit(trainset)\n",
    "\n",
    "#Predict ratings on testset\n",
    "predictions = baseline.test(testset)\n",
    "\n",
    "#Scoring based on RMSE\n",
    "baseline = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Dummy Model has a Root Mean Squared Error (RMSE) of 0.76 meaning that our dummy model predicts ratings that are on average 0.76 points off of the \"true\" ratings of recipes. This is pretty decent but let's see if other models can drive down that score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Only Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "RMSE: 0.5602\n"
     ]
    }
   ],
   "source": [
    "# Instantiate BaselineOnly() from Surprise\n",
    "baseline_only = BaselineOnly()\n",
    "\n",
    "# Fit on the trainset\n",
    "baseline_only.fit(trainset)\n",
    "\n",
    "# Predict ratings on the testset\n",
    "predictions = baseline_only.test(testset)\n",
    "\n",
    "# Scoring based on RMSE\n",
    "baseline_only = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Baseline Only model has an RMSE of 0.56 meaning that our baseline only model predicts ratings that are on average 0.56 points off of the \"true\" ratings of the recipes. This is an improvement over our Dummy Model but can be improved upon using more complicated models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5654\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SVD Model with default hyperparameters\n",
    "svd = SVD(random_state=42)\n",
    "\n",
    "# Fit on the trainset\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Predict ratings on the testset\n",
    "predictions = svd.test(testset)\n",
    "\n",
    "# Scoring based on RMSE\n",
    "SVD_default = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD Model with default hyperparameters has an RMSE of 0.57 meaning that our model predicts ratings that are on average 0.57 points off of the true ratings of the recipes. While this is slightly worse performing than our Baseline Only model, we can adjust hyperparameters to see if it performs better. We will perform a gridsearch to see the best performing RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch with SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 50, 'n_epochs': 30, 'lr_all': 0.0025}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set parameters for the gridsearch\n",
    "SVD_param = {'n_factors':[50,100,150],\n",
    "             #'n_epochs':[10,20,30],\n",
    "            #'lr_all': [.0025, .005, .01]}\n",
    "\n",
    "#Set up the Gridsearch using the parameters above\n",
    "#svd_gridsearch = GridSearchCV(SVD, param_grid = SVD_param, measures = [\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# Fit the gridsearch on the full dataset as the gridsearch will perform splits internally\n",
    "#svd_gridsearch.fit(surprise_data)\n",
    "\n",
    "# Pull out best_params from the gridsearch based on RMSE\n",
    "svd_gridsearch.best_params[\"rmse\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SVD with best_params \n",
    "svd_gs_best_params = SVD(n_factors = 50, n_epochs=30, lr_all = .0025, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5627\n"
     ]
    }
   ],
   "source": [
    "# Fit best params on the trainset\n",
    "svd_gs_best_params.fit(trainset)\n",
    "\n",
    "# Predict ratings on the test set\n",
    "predictions = svd_gs_best_params.test(testset)\n",
    "\n",
    "# Scoring based on RMSE\n",
    "svd_gs_best_params_model = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD Model with best parameters from the gridsearch has. an RMSE of 0.5627 meaning that our model predicts ratings that are on average 0.563 points off of the true ratings of the recipes. This is slightly worse performing than our Baseline Only model. However, we noticed the n_epochs were on the higher end of our bound and our n_factors were at the lower bound, so we can attempt the gridsearch again adjusting these two parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 10, 'n_epochs': 30, 'lr_all': 0.0025}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New parameter dictionary\n",
    "SVD_params_2 = {'n_factors':[10,25,50],\n",
    "             'n_epochs':[30, 40, 50],\n",
    "            'lr_all': [.0025]}\n",
    "\n",
    "# Set up the Gridsearch using the parameters above\n",
    "#svd_gridsearch_2 = GridSearchCV(SVD, param_grid = SVD_params_2, measures = [\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# Fit on the full data \n",
    "#svd_gridsearch_2.fit(surprise_data)\n",
    "\n",
    "# Pull out best_params from the gridsearch based on RMSE\n",
    "#svd_gridsearch_2.best_params[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5600\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SVD with new best_params \n",
    "svd_gs_best_params_model2 = SVD(n_factors = 10 ,  n_epochs= 30  , lr_all = .0025, random_state=42)\n",
    "\n",
    "# Fit model on the trainset data\n",
    "svd_gs_best_params_model2.fit(trainset)\n",
    "\n",
    "\n",
    "# Predict ratings on the testset\n",
    "predictions = svd_gs_best_params_model2.test(testset)\n",
    "\n",
    "# Scoring based on RMSE\n",
    "svd_gs_model_2 = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD Model with best parameters from the gridsearch has. an RMSE of 0.560 meaning that our model predicts ratings that are on average 0.56 points off of the true ratings of the recipes. This is the best performing model so far. We could drop the n_factors since again it is at the lower bounds of the settings, but the difference in RMSE may be minimal with the computation power tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 3, 'n_epochs': 35, 'lr_all': 0.0025}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gridsearch using lower n_factors\n",
    "SVD_params_3 = {'n_factors':[3,5,10],\n",
    "             'n_epochs':[25, 30, 35],\n",
    "            'lr_all': [.0025]}\n",
    "\n",
    "# Set up the Gridsearch using the parameters above\n",
    "#svd_gridsearch_3 = GridSearchCV(SVD, param_grid = SVD_params_3, measures = [\"rmse\", \"mae\"], cv=5)\n",
    "\n",
    "# Fit on the full data\n",
    "#svd_gridsearch_3.fit(surprise_data)\n",
    "\n",
    "# Pull out best_params from the gridsearch based on RMSE\n",
    "#svd_gridsearch_3.best_params[\"rmse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5593\n"
     ]
    }
   ],
   "source": [
    "#Instantiate on SVD model\n",
    "svd_model_3 = SVD(n_factors = 3 ,  n_epochs= 35  , lr_all = .0025, random_state=42)\n",
    "\n",
    "#Fit on trainset\n",
    "svd_model_3.fit(trainset)\n",
    "\n",
    "# Predict for testset\n",
    "predictions = svd_model_3.test(testset)\n",
    "\n",
    "#RMSE \n",
    "svd_model_3 = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD Model with best parameters from the gridsearch has. an RMSE of 0.559 meaning that our model predicts ratings that are on average 0.56 points off of the true ratings of the recipes. This is the best performing model so far, but only slightly beter than our last SVD model. We could drop the n_factors since again it is at the lower bounds of the settings, but the difference in RMSE may be minimal with the computation power tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch with SVDpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then gridsearch with SVDpp, which is the same as SVD except it takes into acount implicit ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5627\n"
     ]
    }
   ],
   "source": [
    "#Instantiate SVDpp with default parameters\n",
    "svdpp = SVDpp(random_state=42)\n",
    "\n",
    "# Fit on trainset\n",
    "svdpp.fit(trainset)\n",
    "\n",
    "#Predict and score\n",
    "predictions = svdpp.test(testset)\n",
    "\n",
    "svdpp_default_model = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-faa291068813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \"lr_all\": [0.0025]}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msvdpp_gs_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVDpp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msvdpp_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msvdpp_gs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurprise_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "#Gridsearching with SVDpp\n",
    "svdpp_param_grid = {'n_factors':[3, 5, 10],\n",
    "                    'n_epochs':[25, 30],\n",
    "                    \"lr_all\": [0.0025, 0.005]}\n",
    "\n",
    "svdpp_gs_model = GridSearchCV(SVDpp, param_grid=svdpp_param_grid, cv=3, verbose=3 )\n",
    "\n",
    "svdpp_gs_model.fit(surprise_data)\n",
    "\n",
    "svdpp_gs_model.best_params[\"rmse\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next try Non-Negative Matrix Factorilization (NMF) models. These models are very similar to SVD models, so we expect them to perform similaraly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6948\n"
     ]
    }
   ],
   "source": [
    "# Instantiate SVD Model with default hyperparameters\n",
    "nmf = NMF(random_state=42)\n",
    "\n",
    "# Fit on the trainset\n",
    "nmf.fit(trainset)\n",
    "\n",
    "# Predict ratings on the testset\n",
    "predictions = nmf.test(testset)\n",
    "\n",
    "# Scoring based on RMSE\n",
    "NMF_default = accuracy.rmse{predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RMSE with the defaul parameters for the NMF model is 0.69 meaning that the model predicts ratings that are on average 0.69 points off of the real ratings. This is so far our worst performing model since the baseline model. Because we have other SVD models that performed better we will not grid search the parameters on this model and look at Surprise's other KNN models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNNBasic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first of the KNN models we will try is the KNN Basic model from Surprise. We are using user_based = True so that the algorithm predicts ratings based on cosine distance from like users rather than like items (recipes).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.6448\n",
      "0.6447560486634945\n"
     ]
    }
   ],
   "source": [
    "sim_cos = ({'name':'cosine', 'user_based':True})\n",
    "knn_basic = KNNBasic(sim_options=sim_cos, random_state=42)\n",
    "\n",
    "# Fit on the trainset\n",
    "knn_basic.fit(trainset)\n",
    "\n",
    "# Predict ratings\n",
    "predictions = knn_basic.test(testset)\n",
    "\n",
    "# RMSE\n",
    "#knn_basic = accuracy.rmse[predictions]\n",
    "print(accuracy.rmse(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performed better than our NMF default parameter model but worse than our SVD, so we will not explore it further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN With Means Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try a KNN With Means model, which operates much like the KNN basic but takes into account the mean rating of each user. Again, we are using user_based = True so that the algorithm predicts ratings based on cosine distance from like users rather than like items (recipes).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.6004\n"
     ]
    }
   ],
   "source": [
    "#from surprise.prediction_algorithms.knns import KNNWithMeans\n",
    "sim_pearson = ({\"name\": \"cosine\", \"user_based\": True})\n",
    "\n",
    "knn_means = KNNWithMeans(sim_options=sim_pearson, random_state=42)\n",
    "\n",
    "# Fit on trainset\n",
    "knn_means.fit(trainset)\n",
    "\n",
    "# predict ratings\n",
    "predictions = knn_means.test(testset)\n",
    "\n",
    "# RMSE\n",
    "knn_means = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6035\n"
     ]
    }
   ],
   "source": [
    "cocluster = CoClustering(random_state=42)\n",
    "\n",
    "# Fit on trainset\n",
    "cocluster.fit(trainset)\n",
    "\n",
    "# Predict on testset\n",
    "predictions = cocluster.test(testset)\n",
    "\n",
    "#RMSE\n",
    "coclusterr_model = accuracy.rmse(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
